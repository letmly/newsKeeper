from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import twitter_samples, stopwords
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
from nltk import FreqDist, classify, NaiveBayesClassifier
import pickle
import re, string, random


def remove_noise(tweet_tokens, stop_words=()):
    cleaned_tokens = []

    for token, tag in pos_tag(tweet_tokens, lang='rus'):
        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\(\),]|' \
                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)
        token = re.sub("(@[A-Za-z0-9_]+)", "", token)

        if tag.startswith("NN"):
            pos = 'n'
        elif tag.startswith('VB'):
            pos = 'v'
        else:
            pos = 'a'

        lemmatizer = WordNetLemmatizer()
        token = lemmatizer.lemmatize(token, pos)

        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:
            cleaned_tokens.append(token.lower())
    return cleaned_tokens


def get_all_words(cleaned_tokens_list):
    for tokens in cleaned_tokens_list:
        for token in tokens:
            yield token


def get_tweets_for_model(cleaned_tokens_list):
    for tweet_tokens in cleaned_tokens_list:
        yield dict([token, True] for token in tweet_tokens)


def get_tokens_from_text(input_text_path):
    with open(input_text_path, 'r', encoding='utf-8') as input_file:
        text = input_file.read()
        lines = text.split('\n')
        res = [word_tokenize(line) for line in lines]
        return res


if __name__ == "__main__":

    # positive_tweets = twitter_samples.strings('positive_tweets.json')
    # negative_tweets = twitter_samples.strings('negative_tweets.json')
    # text = twitter_samples.strings('tweets.20150430-223406.json')
    # tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]

    stop_words = stopwords.words('russian')

    pos_tweet_tokens_train = get_tokens_from_text('pos_prepared_rusentitweet_train.txt')
    pos_tweet_tokens_test = get_tokens_from_text('pos_prepared_rusentitweet_test.txt')
    positive_tweet_tokens = pos_tweet_tokens_train + pos_tweet_tokens_test

    neg_tweet_tokens_train = get_tokens_from_text('neg_prepared_rusentitweet_train.txt')
    neg_tweet_tokens_test = get_tokens_from_text('neg_prepared_rusentitweet_test.txt')
    negative_tweet_tokens = neg_tweet_tokens_train + neg_tweet_tokens_test

    positive_cleaned_tokens_list = []
    negative_cleaned_tokens_list = []

    for tokens in positive_tweet_tokens:
        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))

    for tokens in negative_tweet_tokens:
        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))

    all_pos_words = get_all_words(positive_cleaned_tokens_list)

    freq_dist_pos = FreqDist(all_pos_words)
    print(freq_dist_pos.most_common(10))

    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)
    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)

    positive_dataset = [(tweet_dict, "–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è")
                        for tweet_dict in positive_tokens_for_model]

    negative_dataset = [(tweet_dict, "–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è")
                        for tweet_dict in negative_tokens_for_model]

    dataset = positive_dataset + negative_dataset

    random.shuffle(dataset)
    print(len(dataset))
    train_data = dataset[:int(len(dataset) * .85)]
    test_data = dataset[int(len(dataset) * .85):]

    classifier = NaiveBayesClassifier.train(train_data)

    print("Accuracy is:", classify.accuracy(classifier, test_data))

    print(classifier.show_most_informative_features(10))

    custom_tweets = [
        "–≥–æ—Å–ø–æ–¥–∏ –∫–∞–∫ –∂–µ –∫—Ä—É—Ç–æ, —Ç–∞–∫ –∫—Ä–∞—Å–∏–≤–æ",
        "—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ –∑–∞ –ø–µ—Ä–µ–≤–æ–¥ —Ç—ã –±–æ–ª—å—à–∞—è —É–º–Ω–∏—á–∫–∞üß°",
        "—è –∫—É–ø–∏–ª–∞ —Å–∞–º—ã–π –º–∏–ª—ã–π —á–µ—Ö–æ–ª –Ω–∞ –ø–ª–∞–Ω–µ—Ç–µ –≤—Å–µ–≥–æ –∑–∞ 149 —Ä—É–±–ª–µ–π",
        "–°—É–∫–∞ —è –∑–ª—é—Å—å –Ω–∞ –≤–µ—Å—å –º–∏—Ä",
        '–ì–ª–∞–≤–∞ –í–æ–ª–≥–æ–≥—Ä–∞–¥–∞ –≤–æ–∑–ª–æ–∂–∏–ª —Ü–≤–µ—Ç—ã –∫ –º–µ–º–æ—Ä–∏–∞–ª—å–Ω–æ–π –¥–æ—Å–∫–µ. 29 –¥–µ–∫–∞–±—Ä—è –í–æ–ª–≥–æ–≥—Ä–∞–¥ –∏ –≤—Å—è —Å—Ç—Ä–∞–Ω–∞ –≤—Å–ø–æ–º–∏–Ω–∞—é—Ç —Ç—Ä–∞–≥–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è 2013 –≥–æ–¥–∞. –í —ç—Ç–æ—Ç –¥–µ–Ω—å 10 –ª–µ—Ç –Ω–∞–∑–∞–¥ –≤ –∑–¥–∞–Ω–∏–∏ –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω–æ–≥–æ –≤–æ–∫–∑–∞–ª–∞ –í–æ–ª–≥–æ–≥—Ä–∞–¥–∞ –±—ã–ª —Å–æ–≤–µ—Ä—à–µ–Ω —Ç–µ—Ä–∞–∫—Ç. –í–∑—Ä—ã–≤ –ø—Ä–æ–∏–∑–æ—à–µ–ª –≤ 12:45. –û–Ω –ø—Ä–æ–≥—Ä–µ–º–µ–ª –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–∂–µ –≤–æ–∫–∑–∞–ª–∞ –º–µ–∂–¥—É –≤—Ö–æ–¥–æ–º –≤ –∑–¥–∞–Ω–∏–µ –∏ —Ç—É—Ä–Ω–∏–∫–µ—Ç–æ–º. –¢–µ—Ä–∞–∫—Ç —É–Ω–µ—Å –∂–∏–∑–Ω–∏ 18 —á–µ–ª–æ–≤–µ–∫, –¥–µ—Å—è—Ç–∫–∏ –ª—é–¥–µ–π –ø–æ–ª—É—á–∏–ª–∏ —Ä–∞–Ω–µ–Ω–∏—è. –°–µ–≥–æ–¥–Ω—è, 29 –¥–µ–∫–∞–±—Ä—è, –≥–ª–∞–≤–∞ –í–æ–ª–≥–æ–≥—Ä–∞–¥–∞ –í–ª–∞–¥–∏–º–∏—Ä –ú–∞—Ä—á–µ–Ω–∫–æ –≤–æ–∑–ª–æ–∂–∏–ª —Ü–≤–µ—Ç—ã –∫ –º–µ–º–æ—Ä–∏–∞–ª—å–Ω–æ–π –¥–æ—Å–∫–µ, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–π —É –∂–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω–æ–≥–æ –≤–æ–∫–∑–∞–ª–∞ –í–æ–ª–≥–æ–≥—Ä–∞–¥-I. –û–Ω –ø–æ—á—Ç–∏–ª –º–∏–Ω—É—Ç–æ–π –º–æ–ª—á–∞–Ω–∏—è –ø–∞–º—è—Ç—å –∂–µ—Ä—Ç–≤ —Ç–µ—Ä–∞–∫—Ç–∞. –¢–∞–∫–∂–µ —Å–µ–≥–æ–¥–Ω—è –≥–ª–∞–≤–∞ —Ä–µ–≥–∏–æ–Ω–∞ –ê–Ω–¥—Ä–µ–π –ë–æ—á–∞—Ä–æ–≤ –ø–æ—á—Ç–∏–ª –ø–∞–º—è—Ç—å –ø–æ–≥–∏–±—à–∏—Ö –≤ —Ç–µ—Ä–∞–∫—Ç–∞—Ö 2013 –≥–æ–¥–∞.',
        '–û–Ω–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–∞ —Å—Ç–∞–Ω—Ü–∏–∏ "–ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∞—è" –∏ –≤ —Å–∫–æ—Ä–æ–º –≤—Ä–µ–º–µ–Ω–∏ –±—É–¥—É—Ç –∑–∞–ø—É—â–µ–Ω—ã –≤ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é. –°–µ–≥–æ–¥–Ω—è, 3 —è–Ω–≤–∞—Ä—è, –≤ —Ä–∞–º–∫–∞—Ö –≤—ã–µ–∑–¥–Ω–æ–≥–æ —Ä–∞–±–æ—á–µ–≥–æ —Å–æ–≤–µ—â–∞–Ω–∏—è –Ω–∞ —Å—Ç–∞–Ω—Ü–∏–∏ –º–µ—Ç—Ä–æ—Ç—Ä–∞–º–∞ "–ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∞—è" –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ –ê–Ω–¥—Ä–µ–π –ë–æ—á–∞—Ä–æ–≤ –ª–∏—á–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª –Ω–æ–≤—ã–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —ç—Å–∫–∞–ª–∞—Ç–æ—Ä—ã. –í —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é –æ–Ω–∏ –≤–æ–π–¥—É—Ç –≤ —Å–∞–º–æ–µ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è, –ø–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã. –ù–æ–≤—ã–µ —ç—Å–∫–∞–ª–∞—Ç–æ—Ä—ã —Å–∫–æ—Ä–æ –∑–∞—Ä–∞–±–æ—Ç–∞—é—Ç –∏ –Ω–∞ –¥—Ä—É–≥–æ–π –ø–æ–¥–∑–µ–º–Ω–æ–π —Å—Ç–∞–Ω—Ü–∏–∏ - "–ü–ª–æ—â–∞–¥—å –õ–µ–Ω–∏–Ω–∞". –û–±—Å—É–∂–¥–∞—è –ø—Ä–æ—Ü–µ—Å—Å –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–∞–º–æ–≥–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ–≥–æ –≥–æ—Ä–æ–¥—Å–∫–æ–≥–æ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ –≤ –æ–±–ª–∞—Å—Ç–Ω–æ–π —Å—Ç–æ–ª–∏—Ü–µ, –≥–ª–∞–≤–∞ —Ä–µ–≥–∏–æ–Ω–∞ —Ç–∞–∫–∂–µ –ø–æ—Å—Ç–∞–≤–∏–ª –∑–∞–¥–∞—á—É –ø–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é —Ä–∞–±–æ—Ç—ã —Å—Ç–∞–Ω—Ü–∏–π, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ —Ç—É—Ä–Ω–∏–∫–µ—Ç–æ–≤. - –ú—ã –≤—ã—Ö–æ–¥–∏–º –Ω–∞ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—é –Ω–æ–≤—ã—Ö —ç—Å–∫–∞–ª–∞—Ç–æ—Ä–æ–≤, –Ω–æ –Ω—É–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∏ –¥–∞–ª—å—à–µ, - –∞–∫—Ü–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª –≤–Ω–∏–º–∞–Ω–∏–µ –ê–Ω–¥—Ä–µ–π –ë–æ—á–∞—Ä–æ–≤, - –≤ —Ö–æ–ª–ª–∞—Ö –ø–æ–¥–∑–µ–º–Ω—ã—Ö —Å—Ç–∞–Ω—Ü–∏–π –±–æ–ª—å—à–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –≤ –Ω–∏—Ö –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å —Ç—É—Ä–Ω–∏–∫–µ—Ç—ã. –í —Ü–µ–ª—è—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–µ–ª—å–∑—è –æ—Å—Ç–∞–≤–ª—è—Ç—å –∏ –ª–∞–≤–æ—á–∫–∏ —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ–º –ø–æ–¥ –Ω–∏–º–∏. –ì—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –æ—Ç–º–µ—Ç–∏–ª, —á—Ç–æ –ø–µ—Ä–≤—ã–π —ç—Ç–∞–ø —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–≥–æ —Ç—Ä–∞–º–≤–∞—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω. –î–æ –∫–æ–Ω—Ü–∞ –º–µ—Å—è—Ü–∞ –±—É–¥—É—Ç –∑–∞–≤–µ—Ä—à–µ–Ω—ã –ø—É—Å–∫–æ–Ω–∞–ª–∞–¥–æ—á–Ω—ã–µ —Ä–∞–±–æ—Ç—ã –∏ –Ω–∞—á–Ω—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —ç—Å–∫–∞–ª–∞—Ç–æ—Ä—ã. - –í—Å–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Å—Ç–∞–≤–∏–ª–∏ –Ω–∞ 2023 –≥–æ–¥, –í–æ–ª–≥–æ–≥—Ä–∞–¥—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å –≤—ã–ø–æ–ª–Ω–∏–ª–∞ –≤ –ø–æ–ª–Ω–æ–º –æ–±—ä–µ–º–µ, - —Å–∫–∞–∑–∞–ª –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä. - –≠—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è –∏ –∑–∞–º–µ–Ω—ã —à–ø–∞–ª, —Ä–µ–ª—å—Å–æ–≤, –≤—Å–µ—Ö —Ç–µ—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤ —Ç—Ä–µ—Ö—Å—Ç–æ—Ä–æ–Ω–Ω–µ–º –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–º —Å–æ–≥–ª–∞—à–µ–Ω–∏–∏. –ú—ã –æ—Å—É—â–µ—Å—Ç–≤–∏–ª–∏ –∑–∞–∫—É–ø–∫—É –Ω–æ–≤–æ–≥–æ –ø–æ–¥–≤–∏–∂–Ω–æ–≥–æ —Å–æ—Å—Ç–∞–≤–∞. –í —Ü–µ–ª–æ–º –∑–∞–¥–∞—á–∏ –Ω–∞ 2024 –≥–æ–¥ –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã, –æ–±—ä–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤ –ø–æ–ª–Ω–æ—Å—Ç—å—é –µ—Å—Ç—å. –í —Ç–µ–∫—É—â–µ–º –≥–æ–¥—É –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø—É—Ç–µ–π —Å–∫–æ—Ä–æ—Å—Ç–Ω–æ–≥–æ —Ç—Ä–∞–º–≤–∞—è –æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ ¬´–ü–ª. –í–æ–∑—Ä–æ–∂–¥–µ–Ω–∏—è¬ª –¥–æ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ ¬´–¢–†–ö –ï–≤—Ä–æ–ø–∞¬ª. –¢–∞–∫–∂–µ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω—ã –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏ —Ç—è–≥–æ–≤—ã—Ö –ø–æ–¥—Å—Ç–∞–Ω—Ü–∏–π, –∫–∞–±–µ–ª—å–Ω—ã—Ö –∏ –≤–æ–∑–¥—É—à–Ω—ã—Ö –ª–∏–Ω–∏–π –°–¢, –≤—ã—Ö–æ–¥ –Ω–∞ –ª–∏–Ω–∏—é –∑–∞–∫—É–ø–ª–µ–Ω–Ω—ã—Ö 50 –æ–¥–Ω–æ—Å–µ–∫—Ü–∏–æ–Ω–Ω—ã—Ö –∏ 12 —Ç—Ä–µ—Ö—Å–µ–∫—Ü–∏–æ–Ω–Ω—ã—Ö —Ç—Ä–∞–º–≤–∞–µ–≤;, –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —É—á–∞—Å—Ç–∫–æ–≤ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏ (–Ω–æ–≤–æ–≥–æ —É—á–∞—Å—Ç–∫–∞ –°–¢ –¥–æ –¢–†–ö ¬´–ê–∫–≤–∞—Ä–µ–ª—å¬ª), —Ç—Ä–∞–º–≤–∞–π–Ω—ã—Ö –ª–∏–Ω–∏–π –≤ –ö—Ä–∞—Å–Ω–æ–∞—Ä–º–µ–π—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ.'
    ]
    sentiments = [
        "pos",  # "Just had an amazing experience with AwesomeServices, their customer support is top-notch!"
        "pos",  # "I can't believe the quality of the product I received from FantasticGoods, exceeded my expectations!"
        "pos",  # "Ordered from SuperQuickDelivery and got my package within hours. Impressed!"
        "neg",  # "Had a terrible experience with SlowServiceInc, will never use their services again."
        "neg",  # "In love with the new features of the latest app update. Great job, TechInnovators!"
        "neg",  # "Huge shoutout to DeliciousEats for the tasty food. Will definitely order again!"
    ]
    for i, tweet in enumerate(custom_tweets):
        custom_tokens = remove_noise(word_tokenize(tweet))
        print(f"{i + 1})\n\t{tweet}")
        print("\tPredict: ", classifier.classify(dict([token, True] for token in custom_tokens)))
        print(f"\tReal: {sentiments[i]}\n")

    f = open('my_classifier.pickle', 'wb')
    pickle.dump(classifier, f)
    f.close()

# nltk.download('wordnet')
# nltk.download('averaged_perceptron_tagger_ru')
# nltk.download('stopwords')
# nltk.download('punkt')
